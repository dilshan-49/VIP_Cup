{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bc749a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'FasterRCNN_ResNet18_FPN_Weights' from 'torchvision.models.detection' (e:\\ML\\envs\\torch\\lib\\site-packages\\torchvision\\models\\detection\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdetection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     FasterRCNN_ResNet50_FPN_Weights, \n\u001b[0;32m     11\u001b[0m     FasterRCNN_ResNet18_FPN_Weights\n\u001b[0;32m     12\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'FasterRCNN_ResNet18_FPN_Weights' from 'torchvision.models.detection' (e:\\ML\\envs\\torch\\lib\\site-packages\\torchvision\\models\\detection\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "from torchvision.models.detection.faster_rcnn import FasterRCNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379f9b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0df5f0",
   "metadata": {},
   "source": [
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38ae315",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DroneDataset(Dataset):\n",
    "    def __init__(self, img_dir, label_dir, sample_fraction=1.0):\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        \n",
    "        # Get all image filenames\n",
    "        self.img_files = sorted([f for f in os.listdir(img_dir) if f.endswith(('.jpg', '.png', '.jpeg'))])\n",
    "\n",
    "        # Get all image filenames\n",
    "        all_img_files = sorted([f for f in os.listdir(img_dir) if f.endswith(('.jpg', '.png', '.jpeg'))])\n",
    "        \n",
    "        # Randomly sample a fraction of the dataset\n",
    "        if sample_fraction < 1.0:\n",
    "            num_samples = int(len(all_img_files) * sample_fraction)\n",
    "            np.random.seed(42)  # For reproducibility\n",
    "            indices = np.random.choice(len(all_img_files), num_samples, replace=False)\n",
    "            self.img_files = [all_img_files[i] for i in indices]\n",
    "            print(f\"Using {len(self.img_files)}/{len(all_img_files)} images ({sample_fraction*100:.1f}%)\")\n",
    "        else:\n",
    "            self.img_files = all_img_files        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_files[idx]\n",
    "        \n",
    "        # Load RGB image (320x256)\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Load label\n",
    "        label_name = os.path.splitext(img_name)[0] + '.txt'\n",
    "        label_path = os.path.join(self.label_dir, label_name)\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f.readlines():\n",
    "                    data = line.strip().split()\n",
    "                    class_id = int(data[0])\n",
    "                    # YOLO format: class_id, x_center, y_center, width, height (normalized)\n",
    "                    x_center, y_center, width, height = map(float, data[1:5])\n",
    "                    \n",
    "                    if width <= 0 or height <= 0:\n",
    "                        continue\n",
    "                    \n",
    "                    # Convert to [x_min, y_min, x_max, y_max]\n",
    "                    h, w = image.shape[0], image.shape[1]\n",
    "                    x_min = (x_center - width/2) * w\n",
    "                    y_min = (y_center - height/2) * h\n",
    "                    x_max = (x_center + width/2) * w\n",
    "                    y_max = (y_center + height/2) * h\n",
    "                    \n",
    "                    # Skip boxes that are too small\n",
    "                    if x_max - x_min < 1 or y_max - y_min < 1:\n",
    "                        continue\n",
    "                    \n",
    "                    boxes.append([x_min, y_min, x_max, y_max])\n",
    "                    labels.append(class_id + 1)  # +1 since 0 is background for Faster R-CNN\n",
    "        \n",
    "        # Handle empty boxes case\n",
    "        if len(boxes) == 0:\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels = torch.zeros(0, dtype=torch.int64)\n",
    "            area = torch.zeros(0, dtype=torch.float32)\n",
    "        else:\n",
    "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        \n",
    "        # Convert image to tensor\n",
    "        image = torch.from_numpy(image.transpose(2, 0, 1)).float() / 255.0\n",
    "        \n",
    "        # Create target dictionary\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": torch.tensor([idx]),\n",
    "            \"area\": area,\n",
    "            \"iscrowd\": torch.zeros((len(boxes),), dtype=torch.int64)\n",
    "        }\n",
    "    \n",
    "        \n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11df4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_dataset(dataset, num_samples=5):\n",
    "    \"\"\"Validate dataset by checking a few samples\"\"\"\n",
    "    print(f\"Dataset size: {len(dataset)}\")\n",
    "    \n",
    "    for i in range(min(num_samples, len(dataset))):\n",
    "        try:\n",
    "            image, target = dataset[i]\n",
    "            print(f\"Sample {i}:\")\n",
    "            print(f\"  - Image shape: {image.shape}\")\n",
    "            print(f\"  - Boxes: {target['boxes'].shape}\")\n",
    "            print(f\"  - Labels: {target['labels'].tolist()}\")\n",
    "            print(f\"  - Unique labels: {torch.unique(target['labels']).tolist()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading sample {i}: {e}\")\n",
    "    \n",
    "    print(\"Dataset validation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bd9ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 57580\n",
      "Sample 0:\n",
      "  - Image shape: torch.Size([3, 256, 320])\n",
      "  - Boxes: torch.Size([1, 4])\n",
      "  - Labels: [1]\n",
      "  - Unique labels: [1]\n",
      "Sample 1:\n",
      "  - Image shape: torch.Size([3, 256, 320])\n",
      "  - Boxes: torch.Size([1, 4])\n",
      "  - Labels: [1]\n",
      "  - Unique labels: [1]\n",
      "Sample 2:\n",
      "  - Image shape: torch.Size([3, 256, 320])\n",
      "  - Boxes: torch.Size([1, 4])\n",
      "  - Labels: [1]\n",
      "  - Unique labels: [1]\n",
      "Sample 3:\n",
      "  - Image shape: torch.Size([3, 256, 320])\n",
      "  - Boxes: torch.Size([1, 4])\n",
      "  - Labels: [1]\n",
      "  - Unique labels: [1]\n",
      "Sample 4:\n",
      "  - Image shape: torch.Size([3, 256, 320])\n",
      "  - Boxes: torch.Size([1, 4])\n",
      "  - Labels: [1]\n",
      "  - Unique labels: [1]\n",
      "Dataset validation complete!\n"
     ]
    }
   ],
   "source": [
    "img_dir = \"releasev1-detection&tracking/RGB/images\"\n",
    "label_dir = \"releasev1-detection&tracking/RGB/labels\"\n",
    "\n",
    "dataset = DroneDataset(img_dir=img_dir, label_dir=label_dir)\n",
    "validate_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f689a78",
   "metadata": {},
   "source": [
    "## Loading and Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f52368",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights\n",
    "from torchvision.models.detection import FasterRCNN_MobileNet_V3_Large_FPN_Weights\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.ops import MultiScaleRoIAlign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a5fedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_classes=3, backbone=\"mobilenet\"):\n",
    "    if backbone == \"custom_resnet18\":\n",
    "        # 1. Load pre-trained ResNet-18\n",
    "        backbone = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.DEFAULT)\n",
    "        \n",
    "        # 2. Select layers to use - remove the avg pool and fc layers\n",
    "        backbone = torch.nn.Sequential(*(list(backbone.children())[:-2]))\n",
    "        \n",
    "        # 3. Create FPN on top of it\n",
    "        backbone.out_channels = 512  # ResNet18's last layer channels\n",
    "        \n",
    "        # 4. Create anchor generator\n",
    "        anchor_generator = AnchorGenerator(\n",
    "            sizes=((32, 64, 128, 256, 512),),\n",
    "            aspect_ratios=((0.5, 1.0, 2.0),)\n",
    "        )\n",
    "        \n",
    "        # 5. Create ROI pooler\n",
    "        roi_pooler = MultiScaleRoIAlign(\n",
    "            featmap_names=['0'],\n",
    "            output_size=7,\n",
    "            sampling_ratio=2\n",
    "        )\n",
    "        \n",
    "        # 6. Put everything together\n",
    "        model = FasterRCNN(\n",
    "            backbone,\n",
    "            num_classes=num_classes,\n",
    "            rpn_anchor_generator=anchor_generator,\n",
    "            box_roi_pool=roi_pooler\n",
    "        )\n",
    "    \n",
    "    elif backbone == \"mobilenet\":\n",
    "        # Use MobileNet V3 - fastest pre-built option\n",
    "        weights = FasterRCNN_MobileNet_V3_Large_FPN_Weights.DEFAULT\n",
    "        model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(weights=weights)\n",
    "        \n",
    "        # Modify classifier for your classes\n",
    "        in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    else:\n",
    "        # Original ResNet-50 backbone\n",
    "        weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    "        model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=weights)\n",
    "        \n",
    "        # Modify classifier for your classes\n",
    "        in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dd5ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, num_epochs=20):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Training on {device}\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Parameters\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.SGD(\n",
    "        params, \n",
    "        lr=0.001,\n",
    "        momentum=0.9, \n",
    "        weight_decay=0.0005\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer,\n",
    "        step_size=8,\n",
    "        gamma=0.5\n",
    "    )\n",
    "    \n",
    "    # Track best model\n",
    "    best_loss = float('inf')\n",
    "    best_model_wts = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        total_batches = len(dataloaders['train'])\n",
    "        \n",
    "        # For progress tracking\n",
    "        start_time = time.time()\n",
    "        print_freq = max(1, total_batches // 20)  # Print ~20 updates per epoch\n",
    "        \n",
    "        \n",
    "        for batch_idx, (images, targets) in enumerate(dataloaders['train']):\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            \n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += losses.item()\n",
    "\n",
    "            # Print progress at intervals\n",
    "            if (batch_idx + 1) % print_freq == 0 or (batch_idx + 1) == total_batches:\n",
    "                # Calculate ETA\n",
    "                elapsed_time = time.time() - start_time\n",
    "                batches_per_sec = (batch_idx + 1) / elapsed_time\n",
    "                eta_seconds = (total_batches - batch_idx - 1) / batches_per_sec\n",
    "                \n",
    "                # Format time as hh:mm:ss\n",
    "                eta_str = time.strftime(\"%H:%M:%S\", time.gmtime(eta_seconds))\n",
    "                \n",
    "                # Current loss\n",
    "                current_loss = running_loss / (batch_idx + 1)\n",
    "                \n",
    "                print(f\"Batch {batch_idx+1}/{total_batches} ({(batch_idx+1)/total_batches*100:.1f}%) \"\n",
    "                      f\"Loss: {current_loss:.4f} | ETA: {eta_str}\")\n",
    "                \n",
    "\n",
    "        epoch_loss = running_loss / len(dataloaders['train'])\n",
    "        print(f'Train Loss: {epoch_loss:.4f}')\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, targets in dataloaders['val']:\n",
    "                images = list(image.to(device) for image in images)\n",
    "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "                \n",
    "                # Temporarily switch to train mode to get loss dictionary\n",
    "                model.train()\n",
    "                loss_dict = model(images, targets)\n",
    "                model.eval()  # Switch back to eval mode\n",
    "                \n",
    "                losses = sum(loss for loss in loss_dict.values()) \n",
    "                val_loss += losses.item()\n",
    "        \n",
    "        val_loss = val_loss / len(dataloaders['val'])\n",
    "        print(f'Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_wts = model.state_dict().copy()\n",
    "            torch.save(model.state_dict(), 'best_rgb_drone_detector.pth')\n",
    "        \n",
    "        # Step the scheduler\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3066c29f",
   "metadata": {},
   "source": [
    "## Model Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9785a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_metrics(model, test_loader, device, iou_threshold=0.5):\n",
    "    model.eval()\n",
    "    \n",
    "    all_true_boxes = []\n",
    "    all_true_labels = []\n",
    "    all_pred_boxes = []\n",
    "    all_pred_scores = []\n",
    "    all_pred_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in test_loader:\n",
    "            images = list(img.to(device) for img in images)\n",
    "            \n",
    "            # Get predictions\n",
    "            predictions = model(images)\n",
    "            \n",
    "            # Process each image in the batch\n",
    "            for i, prediction in enumerate(predictions):\n",
    "                # Get ground truth for this image\n",
    "                true_boxes = targets[i]['boxes'].cpu().numpy()\n",
    "                true_labels = targets[i]['labels'].cpu().numpy()\n",
    "                \n",
    "                # Get predictions for this image\n",
    "                pred_boxes = prediction['boxes'].cpu().numpy()\n",
    "                pred_scores = prediction['scores'].cpu().numpy()\n",
    "                pred_labels = prediction['labels'].cpu().numpy()\n",
    "                \n",
    "                # Store for later analysis\n",
    "                all_true_boxes.append(true_boxes)\n",
    "                all_true_labels.append(true_labels)\n",
    "                all_pred_boxes.append(pred_boxes)\n",
    "                all_pred_scores.append(pred_scores)\n",
    "                all_pred_labels.append(pred_labels)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision, recall, f1, mAP = calculate_detection_metrics(\n",
    "        all_true_boxes, all_true_labels, \n",
    "        all_pred_boxes, all_pred_scores, all_pred_labels,\n",
    "        iou_threshold\n",
    "    )\n",
    "    \n",
    "    print(f\"Evaluation Results at IoU={iou_threshold}:\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"mAP: {mAP:.4f}\")\n",
    "    \n",
    "    return precision, recall, f1, mAP\n",
    "\n",
    "def calculate_detection_metrics(true_boxes_list, true_labels_list, \n",
    "                               pred_boxes_list, pred_scores_list, pred_labels_list,\n",
    "                               iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Calculate precision, recall, F1 and mAP for object detection.\n",
    "    This is a simplified version - production code would use libraries like pycocotools.\n",
    "    \"\"\"\n",
    "    total_true_positives = 0\n",
    "    total_false_positives = 0\n",
    "    total_false_negatives = 0\n",
    "    \n",
    "    # For each image\n",
    "    for true_boxes, true_labels, pred_boxes, pred_scores, pred_labels in zip(\n",
    "        true_boxes_list, true_labels_list, pred_boxes_list, pred_scores_list, pred_labels_list):\n",
    "        \n",
    "        # Apply confidence threshold\n",
    "        conf_threshold = 0.5\n",
    "        keep = pred_scores >= conf_threshold\n",
    "        pred_boxes = pred_boxes[keep]\n",
    "        pred_scores = pred_scores[keep]\n",
    "        pred_labels = pred_labels[keep]\n",
    "        \n",
    "        # Track matches\n",
    "        matched = [False] * len(true_boxes)\n",
    "        \n",
    "        # For each prediction (sorted by confidence)\n",
    "        sorted_idx = np.argsort(-pred_scores)\n",
    "        \n",
    "        for idx in sorted_idx:\n",
    "            pred_box = pred_boxes[idx]\n",
    "            pred_label = pred_labels[idx]\n",
    "            \n",
    "            # Check against all ground truth boxes\n",
    "            best_iou = 0\n",
    "            best_gt_idx = -1\n",
    "            \n",
    "            for gt_idx, (true_box, true_label) in enumerate(zip(true_boxes, true_labels)):\n",
    "                # Skip already matched ground truths\n",
    "                if matched[gt_idx]:\n",
    "                    continue\n",
    "                    \n",
    "                # Skip if labels don't match\n",
    "                if pred_label != true_label:\n",
    "                    continue\n",
    "                    \n",
    "                # Calculate IoU\n",
    "                iou = calculate_iou(pred_box, true_box)\n",
    "                \n",
    "                if iou > best_iou and iou >= iou_threshold:\n",
    "                    best_iou = iou\n",
    "                    best_gt_idx = gt_idx\n",
    "            \n",
    "            # If we found a match\n",
    "            if best_gt_idx >= 0:\n",
    "                matched[best_gt_idx] = True\n",
    "                total_true_positives += 1\n",
    "            else:\n",
    "                total_false_positives += 1\n",
    "        \n",
    "        # Count false negatives\n",
    "        total_false_negatives += sum(1 for m in matched if not m)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision = total_true_positives / (total_true_positives + total_false_positives + 1e-6)\n",
    "    recall = total_true_positives / (total_true_positives + total_false_negatives + 1e-6)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-6)\n",
    "    \n",
    "    # Simple mAP calculation (this is simplified)\n",
    "    mAP = precision * recall\n",
    "    \n",
    "    return precision, recall, f1, mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7f05ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"Calculate IoU between two boxes [x1, y1, x2, y2]\"\"\"\n",
    "    # Get intersection coordinates\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    \n",
    "    # Calculate area of intersection\n",
    "    width = max(0, x2 - x1)\n",
    "    height = max(0, y2 - y1)\n",
    "    intersection = width * height\n",
    "    \n",
    "    # Calculate area of both boxes\n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    \n",
    "    # Calculate union\n",
    "    union = box1_area + box2_area - intersection\n",
    "    \n",
    "    # Calculate IoU\n",
    "    iou = intersection / union if union > 0 else 0\n",
    "    return iou\n",
    "\n",
    "def test_inference_speed(model, device, input_size=(3, 256, 320), num_trials=100):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Create dummy input tensor\n",
    "    dummy_input = torch.rand(1, *input_size).to(device)\n",
    "    \n",
    "    # Warm-up\n",
    "    for _ in range(10):\n",
    "        with torch.no_grad():\n",
    "            _ = model([dummy_input])\n",
    "    \n",
    "    # Measure inference time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for _ in range(num_trials):\n",
    "        with torch.no_grad():\n",
    "            _ = model([dummy_input])\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    avg_time = (end_time - start_time) / num_trials\n",
    "    fps = 1.0 / avg_time\n",
    "    \n",
    "    print(f\"Average inference time: {avg_time*1000:.2f} ms\")\n",
    "    print(f\"FPS: {fps:.2f}\")\n",
    "    \n",
    "    return fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21de862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Data paths - update these\n",
    "\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = DroneDataset(img_dir=img_dir, label_dir=label_dir, sample_fraction=0.1)\n",
    "    \n",
    "    # Split dataset (80% train, 20% validation)\n",
    "    dataset_size = len(dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    split = int(0.8 * dataset_size)\n",
    "    \n",
    "    # Use fixed random seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    train_indices = indices[:split]\n",
    "    val_indices = indices[split:]\n",
    "    \n",
    "    # Create data samplers\n",
    "    train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n",
    "    val_sampler = torch.utils.data.SubsetRandomSampler(val_indices)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=4,  # Adjust based on your GPU memory\n",
    "        sampler=train_sampler,\n",
    "        collate_fn=lambda x: tuple(zip(*x))\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=4,\n",
    "        sampler=val_sampler,\n",
    "        collate_fn=lambda x: tuple(zip(*x))\n",
    "    )\n",
    "    \n",
    "    dataloaders = {\n",
    "        'train': train_loader,\n",
    "        'val': val_loader\n",
    "    }\n",
    "    \n",
    "    # Create model (drone + bird + background = 2 classes)\n",
    "    model = get_model(num_classes=3, backbone=\"mobilenet\")\n",
    "    \n",
    "    # Train the model\n",
    "    trained_model = train_model(model, dataloaders, num_epochs=5)\n",
    "    \n",
    "    # Save final model\n",
    "    torch.save(trained_model.state_dict(), 'final_rgb_drone_detector.pth')\n",
    "    \n",
    "    # Test inference speed\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    test_inference_speed(trained_model, device)\n",
    "    \n",
    "    # Evaluate model\n",
    "    evaluate_model_metrics(trained_model, val_loader, device)\n",
    "    \n",
    "    print(\"Training and evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c771ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 5758/57580 images (10.0%)\n",
      "Training on cuda\n",
      "Epoch 1/5\n",
      "----------\n",
      "Batch 57/1152 (4.9%) Loss: 0.3428 | ETA: 00:41:29\n",
      "Batch 114/1152 (9.9%) Loss: 0.3079 | ETA: 00:39:17\n",
      "Batch 171/1152 (14.8%) Loss: 0.2855 | ETA: 00:37:07\n",
      "Batch 228/1152 (19.8%) Loss: 0.2696 | ETA: 00:35:01\n",
      "Batch 285/1152 (24.7%) Loss: 0.2523 | ETA: 00:32:53\n",
      "Batch 342/1152 (29.7%) Loss: 0.2409 | ETA: 00:30:44\n",
      "Batch 399/1152 (34.6%) Loss: 0.2286 | ETA: 00:28:42\n",
      "Batch 456/1152 (39.6%) Loss: 0.2196 | ETA: 00:26:36\n",
      "Batch 513/1152 (44.5%) Loss: 0.2119 | ETA: 00:24:29\n",
      "Batch 570/1152 (49.5%) Loss: 0.2058 | ETA: 00:22:20\n",
      "Batch 627/1152 (54.4%) Loss: 0.2007 | ETA: 00:20:10\n",
      "Batch 684/1152 (59.4%) Loss: 0.1972 | ETA: 00:18:00\n",
      "Batch 741/1152 (64.3%) Loss: 0.1933 | ETA: 00:15:50\n",
      "Batch 798/1152 (69.3%) Loss: 0.1895 | ETA: 00:13:40\n",
      "Batch 855/1152 (74.2%) Loss: 0.1866 | ETA: 00:11:30\n",
      "Batch 912/1152 (79.2%) Loss: 0.1835 | ETA: 00:09:18\n",
      "Batch 969/1152 (84.1%) Loss: 0.1819 | ETA: 00:07:06\n",
      "Batch 1026/1152 (89.1%) Loss: 0.1800 | ETA: 00:04:54\n",
      "Batch 1083/1152 (94.0%) Loss: 0.1780 | ETA: 00:02:41\n",
      "Batch 1140/1152 (99.0%) Loss: 0.1759 | ETA: 00:00:28\n",
      "Batch 1152/1152 (100.0%) Loss: 0.1757 | ETA: 00:00:00\n",
      "Train Loss: 0.1757\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 49\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     46\u001b[0m model \u001b[38;5;241m=\u001b[39m get_model(num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Save final model\u001b[39;00m\n\u001b[0;32m     52\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(trained_model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_rgb_drone_detector.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 86\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, dataloaders, num_epochs)\u001b[0m\n\u001b[0;32m     83\u001b[0m         targets \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[0;32m     85\u001b[0m         loss_dict \u001b[38;5;241m=\u001b[39m model(images, targets)\n\u001b[1;32m---> 86\u001b[0m         losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(loss \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m \u001b[43mloss_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m())\n\u001b[0;32m     88\u001b[0m         val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     90\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m val_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloaders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
