{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "d8bc749a",
      "metadata": {
        "id": "d8bc749a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import cv2\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import time\n",
        "from torchvision.models.detection.faster_rcnn import FasterRCNN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "SVV1s86mZPQy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVV1s86mZPQy",
        "outputId": "4fb0b216-11e4-44ce-ee61-a28576197384"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1OvtlaQjfCmoa7Uso2zNpGHoVE_f2i_9g\n",
            "From (redirected): https://drive.google.com/uc?id=1OvtlaQjfCmoa7Uso2zNpGHoVE_f2i_9g&confirm=t&uuid=ea8b5e7b-d785-46d1-9ef2-3c001e2fda55\n",
            "To: /content/dataset.zip\n",
            "100%|██████████| 761M/761M [00:05<00:00, 137MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted files:\n",
            "['releasev1-detection&tracking']\n"
          ]
        }
      ],
      "source": [
        "# Install gdown to download from Google Drive\n",
        "!pip install -q gdown\n",
        "\n",
        "# Download the ZIP file from Google Drive\n",
        "import gdown\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Google Drive file ID\n",
        "file_id = \"1OvtlaQjfCmoa7Uso2zNpGHoVE_f2i_9g\"\n",
        "zip_output = \"dataset.zip\"\n",
        "\n",
        "# Download using gdown\n",
        "gdown.download(f\"https://drive.google.com/uc?id={file_id}\", output=zip_output, quiet=False)\n",
        "\n",
        "# Extract the ZIP file\n",
        "extract_dir = \"dataset\"\n",
        "with zipfile.ZipFile(zip_output, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "# List extracted files\n",
        "print(\"Extracted files:\")\n",
        "print(os.listdir(extract_dir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "379f9b5a",
      "metadata": {
        "id": "379f9b5a"
      },
      "outputs": [],
      "source": [
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed0df5f0",
      "metadata": {
        "id": "ed0df5f0"
      },
      "source": [
        "## Dataset Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "b38ae315",
      "metadata": {
        "id": "b38ae315"
      },
      "outputs": [],
      "source": [
        "class DroneDataset(Dataset):\n",
        "    def __init__(self, img_dir, label_dir, sample_fraction=1.0):\n",
        "        self.img_dir = img_dir\n",
        "        self.label_dir = label_dir\n",
        "\n",
        "        # Get all image filenames\n",
        "        self.img_files = sorted([f for f in os.listdir(img_dir) if f.endswith(('.jpg', '.png', '.jpeg'))])\n",
        "\n",
        "        # Get all image filenames\n",
        "        all_img_files = sorted([f for f in os.listdir(img_dir) if f.endswith(('.jpg', '.png', '.jpeg'))])\n",
        "\n",
        "        # Randomly sample a fraction of the dataset\n",
        "        if sample_fraction < 1.0:\n",
        "            num_samples = int(len(all_img_files) * sample_fraction)\n",
        "            np.random.seed(42)  # For reproducibility\n",
        "            indices = np.random.choice(len(all_img_files), num_samples, replace=False)\n",
        "            self.img_files = [all_img_files[i] for i in indices]\n",
        "            print(f\"Using {len(self.img_files)}/{len(all_img_files)} images ({sample_fraction*100:.1f}%)\")\n",
        "        else:\n",
        "            self.img_files = all_img_files\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.img_files[idx]\n",
        "\n",
        "        # Load RGB image (320x256)\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Load label\n",
        "        label_name = os.path.splitext(img_name)[0] + '.txt'\n",
        "        label_path = os.path.join(self.label_dir, label_name)\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "\n",
        "        if os.path.exists(label_path):\n",
        "            with open(label_path, 'r') as f:\n",
        "                for line in f.readlines():\n",
        "                    data = line.strip().split()\n",
        "                    class_id = int(data[0])\n",
        "                    # YOLO format: class_id, x_center, y_center, width, height (normalized)\n",
        "                    x_center, y_center, width, height = map(float, data[1:5])\n",
        "\n",
        "                    if width <= 0 or height <= 0:\n",
        "                        continue\n",
        "\n",
        "                    # Convert to [x_min, y_min, x_max, y_max]\n",
        "                    h, w = image.shape[0], image.shape[1]\n",
        "                    x_min = (x_center - width/2) * w\n",
        "                    y_min = (y_center - height/2) * h\n",
        "                    x_max = (x_center + width/2) * w\n",
        "                    y_max = (y_center + height/2) * h\n",
        "\n",
        "                    # Skip boxes that are too small\n",
        "                    if x_max - x_min < 1 or y_max - y_min < 1:\n",
        "                        continue\n",
        "\n",
        "                    boxes.append([x_min, y_min, x_max, y_max])\n",
        "                    labels.append(class_id + 1)  # +1 since 0 is background for Faster R-CNN\n",
        "\n",
        "        # Handle empty boxes case\n",
        "        if len(boxes) == 0:\n",
        "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
        "            labels = torch.zeros(0, dtype=torch.int64)\n",
        "            area = torch.zeros(0, dtype=torch.float32)\n",
        "        else:\n",
        "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "\n",
        "        # Convert image to tensor\n",
        "        image = torch.from_numpy(image.transpose(2, 0, 1)).float() / 255.0\n",
        "\n",
        "        # Create target dictionary\n",
        "        target = {\n",
        "            \"boxes\": boxes,\n",
        "            \"labels\": labels,\n",
        "            \"image_id\": torch.tensor([idx]),\n",
        "            \"area\": area,\n",
        "            \"iscrowd\": torch.zeros((len(boxes),), dtype=torch.int64)\n",
        "        }\n",
        "\n",
        "\n",
        "        return image, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "d11df4f1",
      "metadata": {
        "id": "d11df4f1"
      },
      "outputs": [],
      "source": [
        "def validate_dataset(dataset, num_samples=5):\n",
        "    \"\"\"Validate dataset by checking a few samples\"\"\"\n",
        "    print(f\"Dataset size: {len(dataset)}\")\n",
        "\n",
        "    for i in range(min(num_samples, len(dataset))):\n",
        "        try:\n",
        "            image, target = dataset[i]\n",
        "            print(f\"Sample {i}:\")\n",
        "            print(f\"  - Image shape: {image.shape}\")\n",
        "            print(f\"  - Boxes: {target['boxes'].shape}\")\n",
        "            print(f\"  - Labels: {target['labels'].tolist()}\")\n",
        "            print(f\"  - Unique labels: {torch.unique(target['labels']).tolist()}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading sample {i}: {e}\")\n",
        "\n",
        "    print(\"Dataset validation complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "32bd9ab7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32bd9ab7",
        "outputId": "5f472f41-13fb-4caa-e34c-2e2ed4608f72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset size: 57580\n",
            "Sample 0:\n",
            "  - Image shape: torch.Size([3, 256, 320])\n",
            "  - Boxes: torch.Size([1, 4])\n",
            "  - Labels: [1]\n",
            "  - Unique labels: [1]\n",
            "Sample 1:\n",
            "  - Image shape: torch.Size([3, 256, 320])\n",
            "  - Boxes: torch.Size([1, 4])\n",
            "  - Labels: [1]\n",
            "  - Unique labels: [1]\n",
            "Sample 2:\n",
            "  - Image shape: torch.Size([3, 256, 320])\n",
            "  - Boxes: torch.Size([1, 4])\n",
            "  - Labels: [1]\n",
            "  - Unique labels: [1]\n",
            "Sample 3:\n",
            "  - Image shape: torch.Size([3, 256, 320])\n",
            "  - Boxes: torch.Size([1, 4])\n",
            "  - Labels: [1]\n",
            "  - Unique labels: [1]\n",
            "Sample 4:\n",
            "  - Image shape: torch.Size([3, 256, 320])\n",
            "  - Boxes: torch.Size([1, 4])\n",
            "  - Labels: [1]\n",
            "  - Unique labels: [1]\n",
            "Dataset validation complete!\n"
          ]
        }
      ],
      "source": [
        "img_dir = \"/content/dataset/releasev1-detection&tracking/RGB/images\"\n",
        "label_dir = \"/content/dataset/releasev1-detection&tracking/RGB/labels\"\n",
        "\n",
        "dataset = DroneDataset(img_dir=img_dir, label_dir=label_dir)\n",
        "validate_dataset(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f689a78",
      "metadata": {
        "id": "6f689a78"
      },
      "source": [
        "## Loading and Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "46a5fedc",
      "metadata": {
        "id": "46a5fedc"
      },
      "outputs": [],
      "source": [
        "def get_model(num_classes=3, backbone=\"mobilenet\"):\n",
        "    if backbone == \"custom_resnet18\":\n",
        "        # 1. Load pre-trained ResNet-18\n",
        "        backbone = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.DEFAULT)\n",
        "\n",
        "        # 2. Select layers to use - remove the avg pool and fc layers\n",
        "        backbone = torch.nn.Sequential(*(list(backbone.children())[:-2]))\n",
        "\n",
        "        # 3. Create FPN on top of it\n",
        "        backbone.out_channels = 512  # ResNet18's last layer channels\n",
        "\n",
        "        # 4. Create anchor generator\n",
        "        anchor_generator = AnchorGenerator(\n",
        "            sizes=((32, 64, 128, 256, 512),),\n",
        "            aspect_ratios=((0.5, 1.0, 2.0),)\n",
        "        )\n",
        "\n",
        "        # 5. Create ROI pooler\n",
        "        roi_pooler = MultiScaleRoIAlign(\n",
        "            featmap_names=['0'],\n",
        "            output_size=7,\n",
        "            sampling_ratio=2\n",
        "        )\n",
        "\n",
        "        # 6. Put everything together\n",
        "        model = FasterRCNN(\n",
        "            backbone,\n",
        "            num_classes=num_classes,\n",
        "            rpn_anchor_generator=anchor_generator,\n",
        "            box_roi_pool=roi_pooler\n",
        "        )\n",
        "\n",
        "    elif backbone == \"mobilenet\":\n",
        "        # Use MobileNet V3 - fastest pre-built option\n",
        "        weights = FasterRCNN_MobileNet_V3_Large_FPN_Weights.DEFAULT\n",
        "        model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(weights=weights)\n",
        "\n",
        "        # Freeze backbone layers initially\n",
        "        for param in model.backbone.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Modify classifier for your classes\n",
        "        in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "\n",
        "    else:\n",
        "        # Original ResNet-50 backbone\n",
        "        weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
        "        model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=weights)\n",
        "\n",
        "        # Modify classifier for your classes\n",
        "        in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "c4dd5ab7",
      "metadata": {
        "id": "c4dd5ab7"
      },
      "outputs": [],
      "source": [
        "def train_model(model, dataloaders, num_epochs=20):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Training on {device}\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Parameters\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.SGD(\n",
        "        params,\n",
        "        lr=0.001,\n",
        "        momentum=0.9,\n",
        "        weight_decay=0.0005\n",
        "    )\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "        optimizer,\n",
        "        step_size=8,\n",
        "        gamma=0.5\n",
        "    )\n",
        "\n",
        "\n",
        "    # Track best model\n",
        "    best_loss = float('inf')\n",
        "    best_model_wts = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        total_batches = len(dataloaders['train'])\n",
        "\n",
        "        # For progress tracking\n",
        "        start_time = time.time()\n",
        "        print_freq = max(1, total_batches // 100)  # Print ~100 updates per epoch\n",
        "\n",
        "\n",
        "        for batch_idx, (images, targets) in enumerate(dataloaders['train']):\n",
        "            images = list(image.to(device) for image in images)\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss_dict = model(images, targets)\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "            losses.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += losses.item()\n",
        "\n",
        "            # Print progress at intervals\n",
        "            if (batch_idx + 1) % print_freq == 0 or (batch_idx + 1) == total_batches:\n",
        "                # Calculate ETA\n",
        "                elapsed_time = time.time() - start_time\n",
        "                batches_per_sec = (batch_idx + 1) / elapsed_time\n",
        "                eta_seconds = (total_batches - batch_idx - 1) / batches_per_sec\n",
        "\n",
        "                # Format time as hh:mm:ss\n",
        "                eta_str = time.strftime(\"%H:%M:%S\", time.gmtime(eta_seconds))\n",
        "\n",
        "                # Current loss\n",
        "                current_loss = running_loss / (batch_idx + 1)\n",
        "\n",
        "                print(f\"Batch {batch_idx+1}/{total_batches} ({(batch_idx+1)/total_batches*100:.1f}%) \"\n",
        "                      f\"Loss: {current_loss:.4f} | ETA: {eta_str}\")\n",
        "\n",
        "\n",
        "        epoch_loss = running_loss / len(dataloaders['train'])\n",
        "        print(f'Train Loss: {epoch_loss:.4f}')\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, targets in dataloaders['val']:\n",
        "                images = list(image.to(device) for image in images)\n",
        "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "                # Temporarily switch to train mode to get loss dictionary\n",
        "                model.train()\n",
        "                loss_dict = model(images, targets)\n",
        "                model.eval()  # Switch back to eval mode\n",
        "\n",
        "                losses = sum(loss for loss in loss_dict.values())\n",
        "                val_loss += losses.item()\n",
        "\n",
        "        val_loss = val_loss / len(dataloaders['val'])\n",
        "        print(f'Val Loss: {val_loss:.4f}')\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            best_model_wts = model.state_dict().copy()\n",
        "            torch.save(model.state_dict(), 'best_rgb_drone_detector.pth')\n",
        "\n",
        "        # Step the scheduler\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        print()\n",
        "\n",
        "    # Load best model\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3066c29f",
      "metadata": {
        "id": "3066c29f"
      },
      "source": [
        "## Model Evaluations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "f9785a30",
      "metadata": {
        "id": "f9785a30"
      },
      "outputs": [],
      "source": [
        "def evaluate_model_metrics(model, test_loader, device, iou_threshold=0.5):\n",
        "    model.eval()\n",
        "\n",
        "    all_true_boxes = []\n",
        "    all_true_labels = []\n",
        "    all_pred_boxes = []\n",
        "    all_pred_scores = []\n",
        "    all_pred_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in test_loader:\n",
        "            images = list(img.to(device) for img in images)\n",
        "\n",
        "            # Get predictions\n",
        "            predictions = model(images)\n",
        "\n",
        "            # Process each image in the batch\n",
        "            for i, prediction in enumerate(predictions):\n",
        "                # Get ground truth for this image\n",
        "                true_boxes = targets[i]['boxes'].cpu().numpy()\n",
        "                true_labels = targets[i]['labels'].cpu().numpy()\n",
        "\n",
        "                # Get predictions for this image\n",
        "                pred_boxes = prediction['boxes'].cpu().numpy()\n",
        "                pred_scores = prediction['scores'].cpu().numpy()\n",
        "                pred_labels = prediction['labels'].cpu().numpy()\n",
        "\n",
        "                # Store for later analysis\n",
        "                all_true_boxes.append(true_boxes)\n",
        "                all_true_labels.append(true_labels)\n",
        "                all_pred_boxes.append(pred_boxes)\n",
        "                all_pred_scores.append(pred_scores)\n",
        "                all_pred_labels.append(pred_labels)\n",
        "\n",
        "    # Calculate metrics\n",
        "    precision, recall, f1, mAP = calculate_detection_metrics(\n",
        "        all_true_boxes, all_true_labels,\n",
        "        all_pred_boxes, all_pred_scores, all_pred_labels,\n",
        "        iou_threshold\n",
        "    )\n",
        "\n",
        "    print(f\"Evaluation Results at IoU={iou_threshold}:\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"mAP: {mAP:.4f}\")\n",
        "\n",
        "    return precision, recall, f1, mAP\n",
        "\n",
        "def calculate_detection_metrics(true_boxes_list, true_labels_list,\n",
        "                               pred_boxes_list, pred_scores_list, pred_labels_list,\n",
        "                               iou_threshold=0.5):\n",
        "    \"\"\"\n",
        "    Calculate precision, recall, F1 and mAP for object detection.\n",
        "    This is a simplified version - production code would use libraries like pycocotools.\n",
        "    \"\"\"\n",
        "    total_true_positives = 0\n",
        "    total_false_positives = 0\n",
        "    total_false_negatives = 0\n",
        "\n",
        "    # For each image\n",
        "    for true_boxes, true_labels, pred_boxes, pred_scores, pred_labels in zip(\n",
        "        true_boxes_list, true_labels_list, pred_boxes_list, pred_scores_list, pred_labels_list):\n",
        "\n",
        "        # Apply confidence threshold\n",
        "        conf_threshold = 0.5\n",
        "        keep = pred_scores >= conf_threshold\n",
        "        pred_boxes = pred_boxes[keep]\n",
        "        pred_scores = pred_scores[keep]\n",
        "        pred_labels = pred_labels[keep]\n",
        "\n",
        "        # Track matches\n",
        "        matched = [False] * len(true_boxes)\n",
        "\n",
        "        # For each prediction (sorted by confidence)\n",
        "        sorted_idx = np.argsort(-pred_scores)\n",
        "\n",
        "        for idx in sorted_idx:\n",
        "            pred_box = pred_boxes[idx]\n",
        "            pred_label = pred_labels[idx]\n",
        "\n",
        "            # Check against all ground truth boxes\n",
        "            best_iou = 0\n",
        "            best_gt_idx = -1\n",
        "\n",
        "            for gt_idx, (true_box, true_label) in enumerate(zip(true_boxes, true_labels)):\n",
        "                # Skip already matched ground truths\n",
        "                if matched[gt_idx]:\n",
        "                    continue\n",
        "\n",
        "                # Skip if labels don't match\n",
        "                if pred_label != true_label:\n",
        "                    continue\n",
        "\n",
        "                # Calculate IoU\n",
        "                iou = calculate_iou(pred_box, true_box)\n",
        "\n",
        "                if iou > best_iou and iou >= iou_threshold:\n",
        "                    best_iou = iou\n",
        "                    best_gt_idx = gt_idx\n",
        "\n",
        "            # If we found a match\n",
        "            if best_gt_idx >= 0:\n",
        "                matched[best_gt_idx] = True\n",
        "                total_true_positives += 1\n",
        "            else:\n",
        "                total_false_positives += 1\n",
        "\n",
        "        # Count false negatives\n",
        "        total_false_negatives += sum(1 for m in matched if not m)\n",
        "\n",
        "    # Calculate metrics\n",
        "    precision = total_true_positives / (total_true_positives + total_false_positives + 1e-6)\n",
        "    recall = total_true_positives / (total_true_positives + total_false_negatives + 1e-6)\n",
        "    f1 = 2 * precision * recall / (precision + recall + 1e-6)\n",
        "\n",
        "    # Simple mAP calculation (this is simplified)\n",
        "    mAP = precision * recall\n",
        "\n",
        "    return precision, recall, f1, mAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "8a7f05ff",
      "metadata": {
        "id": "8a7f05ff"
      },
      "outputs": [],
      "source": [
        "def calculate_iou(box1, box2):\n",
        "    \"\"\"Calculate IoU between two boxes [x1, y1, x2, y2]\"\"\"\n",
        "    # Get intersection coordinates\n",
        "    x1 = max(box1[0], box2[0])\n",
        "    y1 = max(box1[1], box2[1])\n",
        "    x2 = min(box1[2], box2[2])\n",
        "    y2 = min(box1[3], box2[3])\n",
        "\n",
        "    # Calculate area of intersection\n",
        "    width = max(0, x2 - x1)\n",
        "    height = max(0, y2 - y1)\n",
        "    intersection = width * height\n",
        "\n",
        "    # Calculate area of both boxes\n",
        "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
        "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
        "\n",
        "    # Calculate union\n",
        "    union = box1_area + box2_area - intersection\n",
        "\n",
        "    # Calculate IoU\n",
        "    iou = intersection / union if union > 0 else 0\n",
        "    return iou\n",
        "\n",
        "def test_inference_speed(model, device, input_size=(3, 256, 320), num_trials=100):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Create dummy input tensor\n",
        "    dummy_input = torch.rand(1, *input_size).to(device)\n",
        "\n",
        "    # Warm-up\n",
        "    for _ in range(10):\n",
        "        with torch.no_grad():\n",
        "            _ = model([dummy_input])\n",
        "\n",
        "    # Measure inference time\n",
        "    start_time = time.time()\n",
        "\n",
        "    for _ in range(num_trials):\n",
        "        with torch.no_grad():\n",
        "            _ = model([dummy_input])\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    avg_time = (end_time - start_time) / num_trials\n",
        "    fps = 1.0 / avg_time\n",
        "\n",
        "    print(f\"Average inference time: {avg_time*1000:.2f} ms\")\n",
        "    print(f\"FPS: {fps:.2f}\")\n",
        "\n",
        "    return fps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "f21de862",
      "metadata": {
        "id": "f21de862"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    # Data paths - update these\n",
        "\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = DroneDataset(img_dir=img_dir, label_dir=label_dir, sample_fraction=0.6)\n",
        "\n",
        "    # Split dataset (80% train, 20% validation)\n",
        "    dataset_size = len(dataset)\n",
        "    indices = list(range(dataset_size))\n",
        "    split = int(0.8 * dataset_size)\n",
        "\n",
        "    # Use fixed random seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    train_indices = indices[:split]\n",
        "    val_indices = indices[split:]\n",
        "\n",
        "    # Create data samplers\n",
        "    train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n",
        "    val_sampler = torch.utils.data.SubsetRandomSampler(val_indices)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=16,  # Adjust based on your GPU memory\n",
        "        sampler=train_sampler,\n",
        "        collate_fn=lambda x: tuple(zip(*x))\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=16,\n",
        "        sampler=val_sampler,\n",
        "        collate_fn=lambda x: tuple(zip(*x))\n",
        "    )\n",
        "\n",
        "    dataloaders = {\n",
        "        'train': train_loader,\n",
        "        'val': val_loader\n",
        "    }\n",
        "\n",
        "    # Create model (drone + bird + background = 2 classes)\n",
        "    model = get_model(num_classes=3, backbone=\"custom_resnet18\")\n",
        "\n",
        "    # Train the model\n",
        "    trained_model = train_model(model, dataloaders, num_epochs=5)\n",
        "\n",
        "    # Save final model\n",
        "    torch.save(trained_model.state_dict(), 'final_rgb_drone_detector.pth')\n",
        "\n",
        "    # Test inference speed\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    test_inference_speed(trained_model, device)\n",
        "\n",
        "    # Evaluate model\n",
        "    evaluate_model_metrics(trained_model, val_loader, device)\n",
        "\n",
        "    print(\"Training and evaluation complete!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
