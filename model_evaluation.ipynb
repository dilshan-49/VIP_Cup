{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d47c0ad",
   "metadata": {},
   "source": [
    "# Faster R-CNN Model Evaluation\n",
    "\n",
    "This notebook evaluates Faster R-CNN models with ResNet-18 and ResNet-50 backbones, compares their performance, and visualizes predictions alongside ground truths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a40e93",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "970585c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.transforms import functional as F\n",
    "import time\n",
    "import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f33ed4",
   "metadata": {},
   "source": [
    "## Load Pretrained Models\n",
    "\n",
    "Load our fine-tuned Faster R-CNN models with ResNet-18 and ResNet-50 backbones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cd6810d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\ML\\envs\\torch\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "e:\\ML\\envs\\torch\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded ResNet-18 model from e:/ML/VIP_Cup/models/best_detector_resnet18.pth\n",
      "Successfully loaded ResNet-50 model from e:/ML/VIP_Cup/models/best_detector_resnet50.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=3, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=12, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Function to create ResNet18 Faster R-CNN model\n",
    "def get_fasterrcnn_resnet18(num_classes):\n",
    "    # Load a pre-trained Faster R-CNN model with ResNet18 backbone\n",
    "    # 1. Load pre-trained ResNet-18\n",
    "    backbone = torchvision.models.resnet18(pretrained=True)  # pretrained=True for older PyTorch versions\n",
    "\n",
    "    # 2. Select layers to use - remove the avg pool and fc layers\n",
    "    backbone = torch.nn.Sequential(*(list(backbone.children())[:-2]))\n",
    "    \n",
    "    # 3. Create FPN on top of it\n",
    "    backbone.out_channels = 512  # ResNet18's last layer channels\n",
    "\n",
    "    # 4. Create anchor generator\n",
    "    anchor_generator = torchvision.models.detection.rpn.AnchorGenerator(\n",
    "        sizes=((32, 64, 128, 256, 512),),\n",
    "        aspect_ratios=((0.5, 1.0, 2.0),)\n",
    "    )\n",
    "\n",
    "    # 5. Create ROI pooler\n",
    "    roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n",
    "        featmap_names=['0'],\n",
    "        output_size=7,\n",
    "        sampling_ratio=2\n",
    "    )\n",
    "\n",
    "    # 6. Put everything together\n",
    "    model = torchvision.models.detection.FasterRCNN(\n",
    "        backbone,\n",
    "        num_classes=num_classes,\n",
    "        rpn_anchor_generator=anchor_generator,\n",
    "        box_roi_pool=roi_pooler\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Function to create ResNet50 Faster R-CNN model\n",
    "def get_fasterrcnn_resnet50(num_classes):\n",
    "    # Load a pre-trained Faster R-CNN model with ResNet50 backbone\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "    \n",
    "    # Replace the classifier with a new one for our number of classes\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Define number of classes (background + object classes)\n",
    "num_classes = 3  # Example: 20 PASCAL VOC classes + background\n",
    "\n",
    "# Load the trained models\n",
    "resnet18_model_path = 'e:/ML/VIP_Cup/models/best_detector_resnet18.pth'\n",
    "resnet50_model_path = 'e:/ML/VIP_Cup/models/best_detector_resnet50.pth'\n",
    "\n",
    "# Initialize models\n",
    "resnet18_model = get_fasterrcnn_resnet18(num_classes)\n",
    "resnet50_model = get_fasterrcnn_resnet50(num_classes)\n",
    "\n",
    "# Load trained weights\n",
    "try:\n",
    "    resnet18_model.load_state_dict(torch.load(resnet18_model_path, map_location=device))\n",
    "    print(f\"Successfully loaded ResNet-18 model from {resnet18_model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading ResNet-18 model: {e}\")\n",
    "\n",
    "try:\n",
    "    resnet50_model.load_state_dict(torch.load(resnet50_model_path, map_location=device))\n",
    "    print(f\"Successfully loaded ResNet-50 model from {resnet50_model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading ResNet-50 model: {e}\")\n",
    "\n",
    "# Set models to evaluation mode\n",
    "resnet18_model.to(device).eval()\n",
    "resnet50_model.to(device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b37e64",
   "metadata": {},
   "source": [
    "## Load Dataset and Ground Truths\n",
    "\n",
    "Load the RGB dataset from releasev1-detection&tracking folder, where images are in the images folder and annotations are in the labels folder as txt files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a487fc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 57580 images in e:/ML/VIP_Cup/releasev1-detection&tracking/RGB\\images\n",
      "Randomly sampled 5758 images from a total of 57580 images\n",
      "Sample data from the dataset:\n",
      "Sample 0:\n",
      "  - Image shape: torch.Size([3, 256, 320])\n",
      "  - Number of objects: 1\n",
      "  - Labels: [2]\n",
      "    Object 0: bird, Box: [181.5, 112.5, 207.5, 131.5]\n",
      "Sample 1:\n",
      "  - Image shape: torch.Size([3, 256, 320])\n",
      "  - Number of objects: 1\n",
      "  - Labels: [1]\n",
      "    Object 0: drone, Box: [108.5, 191.5, 125.5, 204.5]\n",
      "Sample 2:\n",
      "  - Image shape: torch.Size([3, 256, 320])\n",
      "  - Number of objects: 1\n",
      "  - Labels: [2]\n",
      "    Object 0: bird, Box: [215.0, 139.0, 229.0, 148.0]\n"
     ]
    }
   ],
   "source": [
    "# Define the class names for drone and bird detection\n",
    "class_names = ['background', 'drone', 'bird']\n",
    "\n",
    "# Define paths for the dataset\n",
    "data_root = 'e:/ML/VIP_Cup/releasev1-detection&tracking/RGB'\n",
    "img_dir = os.path.join(data_root, 'images')\n",
    "label_dir = os.path.join(data_root, 'labels')\n",
    "\n",
    "# Check if the directories exist\n",
    "if not os.path.exists(img_dir):\n",
    "    print(f\"Warning: Image directory not found at {img_dir}\")\n",
    "if not os.path.exists(label_dir):\n",
    "    print(f\"Warning: Label directory not found at {label_dir}\")\n",
    "\n",
    "# Get all image files\n",
    "image_files = sorted([os.path.join(img_dir, f) for f in os.listdir(img_dir)\n",
    "                     if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
    "\n",
    "print(f\"Found {len(image_files)} images in {img_dir}\")\n",
    "\n",
    "# Create a dataset class similar to what was used in training\n",
    "class DroneDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_dir, label_dir, sample_fraction=1.0):\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        all_img_files = sorted([f for f in os.listdir(img_dir) if f.endswith(('.jpg', '.png', '.jpeg'))])\n",
    "     \n",
    "        if sample_fraction < 1.0:\n",
    "            num_samples = int(len(all_img_files) * sample_fraction)\n",
    "            # Use random seed for reproducibility\n",
    "            np.random.seed(42)\n",
    "            indices = np.random.choice(len(all_img_files), size=num_samples, replace=False)\n",
    "            self.img_files = [all_img_files[i] for i in indices]\n",
    "            print(f\"Randomly sampled {len(self.img_files)} images from a total of {len(all_img_files)} images\")\n",
    "        else:\n",
    "            self.img_files = all_img_files\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_files[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        \n",
    "        # Load image\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        height, width = image.shape[:2]\n",
    "        \n",
    "        # Load labels from txt file (YOLO format)\n",
    "        label_name = os.path.splitext(img_name)[0] + '.txt'\n",
    "        label_path = os.path.join(self.label_dir, label_name)\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f.readlines():\n",
    "                    data = line.strip().split()\n",
    "                    class_id = int(data[0])\n",
    "                    # YOLO format: class_id, x_center, y_center, width, height (normalized)\n",
    "                    x_center, y_center, w, h = map(float, data[1:5])\n",
    "                    \n",
    "                    # Convert to [x_min, y_min, x_max, y_max]\n",
    "                    x_min = (x_center - w/2) * width\n",
    "                    y_min = (y_center - h/2) * height\n",
    "                    x_max = (x_center + w/2) * width\n",
    "                    y_max = (y_center + h/2) * height\n",
    "                    \n",
    "                    boxes.append([x_min, y_min, x_max, y_max])\n",
    "                    labels.append(class_id + 1)  # +1 since 0 is background for Faster R-CNN\n",
    "        \n",
    "        # Handle empty boxes case\n",
    "        if len(boxes) == 0:\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels = torch.zeros(0, dtype=torch.int64)\n",
    "        else:\n",
    "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        \n",
    "        # Convert image to tensor\n",
    "        image = torch.from_numpy(image.transpose(2, 0, 1)).float() / 255.0\n",
    "        \n",
    "        # Get image id from filename\n",
    "        image_id = torch.from_numpy(np.array([idx])).long()\n",
    "        \n",
    "        # Calculate area\n",
    "        if len(boxes) > 0:\n",
    "            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        else:\n",
    "            area = torch.zeros(0, dtype=torch.float32)\n",
    "            \n",
    "        # Create target dictionary\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": image_id,\n",
    "            \"area\": area,\n",
    "            \"iscrowd\": torch.zeros((len(boxes),), dtype=torch.int64),\n",
    "            \"orig_filename\": img_name\n",
    "        }\n",
    "        \n",
    "        return image, target\n",
    "\n",
    "# Create dataset and dataloader\n",
    "test_dataset = DroneDataset(img_dir, label_dir, 0.1)\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda x: tuple(zip(*x))  # This is required for variable size boxes\n",
    ")\n",
    "\n",
    "# Check a few samples\n",
    "print(\"Sample data from the dataset:\")\n",
    "for i in range(min(3, len(test_dataset))):\n",
    "    image, target = test_dataset[i]\n",
    "    print(f\"Sample {i}:\")\n",
    "    print(f\"  - Image shape: {image.shape}\")\n",
    "    print(f\"  - Number of objects: {len(target['boxes'])}\")\n",
    "    if len(target['boxes']) > 0:\n",
    "        print(f\"  - Labels: {target['labels'].tolist()}\")\n",
    "        for j, label in enumerate(target['labels'].tolist()):\n",
    "            class_name = class_names[label] if label < len(class_names) else f\"Class {label}\"\n",
    "            print(f\"    Object {j}: {class_name}, Box: {target['boxes'][j].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3406f1",
   "metadata": {},
   "source": [
    "## Evaluate Models\n",
    "\n",
    "Run both models on the dataset and compute evaluation metrics such as mAP (mean Average Precision)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a1f818",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(model, dataloader, num_images=None ):\n",
    "    \"\"\"Evaluate a model on a set of images.\"\"\"\n",
    "    results = []\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    # Calculate how many batches to process\n",
    "    total_batches = len(dataloader)\n",
    "    if num_images:\n",
    "        # Calculate how many batches we need for the requested number of images\n",
    "        batch_size = dataloader.batch_size\n",
    "        total_batches = min(total_batches, (num_images + batch_size - 1) // batch_size)\n",
    "    \n",
    "    # Create progress bar\n",
    "    progress_bar = tqdm.tqdm(total=total_batches, desc=\"Evaluating model\", \n",
    "                           position=0, leave=True, ncols=100)\n",
    "\n",
    "    image_count = 0\n",
    "    batch_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in dataloader:\n",
    "            batch_count += 1\n",
    "            if batch_count > total_batches:\n",
    "                break            \n",
    "            \n",
    "            device_images = [img.to(device) for img in images]\n",
    "            predictions = model(device_images)\n",
    "               \n",
    "            # Process results for each image in the batch\n",
    "            for img_idx, (prediction, target) in enumerate(zip(predictions, targets)):\n",
    "                if num_images and image_count >= num_images:\n",
    "                    break\n",
    "                    \n",
    "                image_id = target[\"image_id\"].item()\n",
    "                filename = target[\"orig_filename\"]\n",
    "                # Get predictions above threshold\n",
    "                threshold = 0.5\n",
    "                keep_indices = prediction['scores'] >= threshold\n",
    "                \n",
    "                boxes = prediction['boxes'][keep_indices].cpu().numpy()\n",
    "                scores = prediction['scores'][keep_indices].cpu().numpy()\n",
    "                labels = prediction['labels'][keep_indices].cpu().numpy()\n",
    "                \n",
    "                # Store results for each detection\n",
    "                for box, score, label in zip(boxes, scores, labels):\n",
    "                    x1, y1, x2, y2 = box\n",
    "                    width = x2 - x1\n",
    "                    height = y2 - y1\n",
    "                    \n",
    "                    results.append({\n",
    "                        'image_id': image_id,\n",
    "                        'filename': filename,\n",
    "                        'category_id': int(label),\n",
    "                        'bbox': [float(x1), float(y1), float(width), float(height)],\n",
    "                        'score': float(score)\n",
    "                    })\n",
    "                \n",
    "                image_count += 1\n",
    "                \n",
    "            # Update progress bar\n",
    "            progress_bar.update(1)\n",
    "            \n",
    "            # Break if we've processed enough images\n",
    "            if num_images and image_count >= num_images:\n",
    "                break\n",
    "    \n",
    "    progress_bar.close()\n",
    "    print(f\"Evaluated {image_count} images, found {len(results)} detections\")\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate ResNet-18 model\n",
    "print(\"Evaluating Faster R-CNN with ResNet-18 backbone...\")\n",
    "try:\n",
    "    resnet18_results = evaluate_model(resnet18_model, test_dataloader)  # Limit to 10 images for demonstration\n",
    "    print(\"ResNet-18 evaluation completed.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error evaluating ResNet-18 model: {e}\")\n",
    "\n",
    "# Evaluate ResNet-50 model\n",
    "print(\"\\nEvaluating Faster R-CNN with ResNet-50 backbone...\")\n",
    "#try:\n",
    "resnet50_results = evaluate_model(resnet50_model, test_dataloader)  # Limit to 10 images for demonstration\n",
    "print(\"ResNet-50 evaluation completed.\")\n",
    "#except Exception as e:\n",
    "#    print(f\"Error evaluating ResNet-50 model: {e}\")\n",
    "\n",
    "# Compare results\n",
    "print(\"\\nComparison of results:\")\n",
    "for i in range(len(resnet18_results)):\n",
    "    resnet18_score = resnet18_results[i]['score']\n",
    "    resnet50_score = resnet50_results[i]['score']\n",
    "    image_id = resnet18_results[i]['image_id']\n",
    "    \n",
    "    print(f\"Image ID {image_id}: ResNet-18 Score = {resnet18_score:.4f}, ResNet-50 Score = {resnet50_score:.4f}\")\n",
    "    if resnet50_score > resnet18_score:\n",
    "        print(f\"  -> ResNet-50 performed better on this image.\")\n",
    "    elif resnet18_score > resnet50_score:\n",
    "        print(f\"  -> ResNet-18 performed better on this image.\")\n",
    "    else:\n",
    "        print(f\"  -> Both models performed equally on this image.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63a6c02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8171\n"
     ]
    }
   ],
   "source": [
    "print(len(resnet18_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598e81f8",
   "metadata": {},
   "source": [
    "## Visualize Predictions and Ground Truths\n",
    "\n",
    "Plot images with model predictions (bounding boxes and labels) alongside ground truth annotations for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b0ee2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_combined(image_path, model_results, ground_truth, model_name=\"Model\", class_names=None):\n",
    "    \"\"\"\n",
    "    Visualize detections and ground truth on the same image with different colored boxes\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the image\n",
    "        model_results: List of detection dictionaries for this image\n",
    "        ground_truth: Ground truth annotation dictionary for this image\n",
    "        model_name: Name to display for the model\n",
    "        class_names: List of class names for label display\n",
    "    \"\"\"\n",
    "    if class_names is None:\n",
    "        class_names = ['background', 'bird', 'drone']\n",
    "    \n",
    "    # Load image\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Error: Could not load image at {image_path}\")\n",
    "        return None\n",
    "        \n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "    \n",
    "    # Display image\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"{model_name} Predictions & Ground Truth - {os.path.basename(image_path)}\")\n",
    "    \n",
    "    # Plot ground truth boxes in green\n",
    "    gt_boxes = ground_truth[\"boxes\"]\n",
    "    gt_labels = ground_truth[\"labels\"]\n",
    "    \n",
    "    for box, label_idx in zip(gt_boxes, gt_labels):\n",
    "        x1, y1, x2, y2 = box.tolist()\n",
    "        width = x2 - x1\n",
    "        height = y2 - y1\n",
    "        \n",
    "        # Adjust class index since background=0\n",
    "        class_idx = label_idx.item()\n",
    "        class_name = class_names[class_idx] if class_idx < len(class_names) else f\"Class {class_idx}\"\n",
    "        \n",
    "        # Create rectangle patch with green color for ground truth\n",
    "        rect = patches.Rectangle((x1, y1), width, height, \n",
    "                               linewidth=2, edgecolor='green', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Add label with a solid green box background for ground truth\n",
    "        ax.text(x1, y1-5, f\"GT: {class_name}\", color='white', \n",
    "                fontsize=10, bbox=dict(facecolor='green', alpha=0.7))\n",
    "    \n",
    "    # Plot predicted boxes in blue\n",
    "    for detection in model_results:\n",
    "        x1, y1, width, height = detection['bbox']\n",
    "        class_idx = detection['category_id']\n",
    "        score = detection['score']\n",
    "        \n",
    "        class_name = class_names[class_idx] if class_idx < len(class_names) else f\"Class {class_idx}\"\n",
    "        \n",
    "        # Create rectangle patch with blue color for predictions\n",
    "        rect = patches.Rectangle((x1, y1), width, height, \n",
    "                               linewidth=2, edgecolor='blue', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Add label with confidence score and a solid blue box background for predictions\n",
    "        ax.text(x1, y1+height+15, f\"Pred: {class_name} ({score:.2f})\", color='white', \n",
    "                fontsize=10, bbox=dict(facecolor='blue', alpha=0.7))\n",
    "    \n",
    "    # Add a legend\n",
    "    legend_elements = [\n",
    "        patches.Patch(edgecolor='green', facecolor='none', linewidth=2, label='Ground Truth'),\n",
    "        patches.Patch(edgecolor='blue', facecolor='none', linewidth=2, label='Model Prediction')\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def create_combined_visualizations(dataset, resnet18_results, resnet50_results, num_samples=5):\n",
    "    \"\"\"\n",
    "    Create combined visualizations showing both ground truth and predictions on the same image\n",
    "    \"\"\"\n",
    "    # Get image paths and convert to dictionary for easier lookup\n",
    "    img_paths = {}\n",
    "    for img_name in dataset.img_files:\n",
    "        img_path = os.path.join(dataset.img_dir, img_name)\n",
    "        img_paths[img_name] = img_path\n",
    "    \n",
    "    # Group results by image_id for easier lookup\n",
    "    resnet18_by_id = {}\n",
    "    for result in resnet18_results:\n",
    "        image_id = result['image_id']\n",
    "        if image_id not in resnet18_by_id:\n",
    "            resnet18_by_id[image_id] = []\n",
    "        resnet18_by_id[image_id].append(result)\n",
    "    \n",
    "    resnet50_by_id = {}\n",
    "    for result in resnet50_results:\n",
    "        image_id = result['image_id']\n",
    "        if image_id not in resnet50_by_id:\n",
    "            resnet50_by_id[image_id] = []\n",
    "        resnet50_by_id[image_id].append(result)\n",
    "    \n",
    "    # Find image IDs with detections from both models\n",
    "    common_ids = list(set(resnet18_by_id.keys()) & set(resnet50_by_id.keys()))\n",
    "    \n",
    "    # Select random samples to visualize\n",
    "    if len(common_ids) > num_samples:\n",
    "        sample_ids = np.random.choice(common_ids, size=num_samples, replace=False)\n",
    "    else:\n",
    "        sample_ids = common_ids[:num_samples]\n",
    "    \n",
    "    # Create visualizations for each sample\n",
    "    for idx, image_id in enumerate(sample_ids):\n",
    "        # Get ground truth for this image\n",
    "        image, target = dataset[image_id]\n",
    "        filename = target['orig_filename']\n",
    "        img_path = img_paths[filename]\n",
    "        \n",
    "        print(f\"\\nVisualization {idx+1}/{len(sample_ids)}: Image ID {image_id} ({filename})\")\n",
    "        \n",
    "        # Visualize ResNet-18 results\n",
    "        print(\"ResNet-18 model predictions with ground truth:\")\n",
    "        fig1 = visualize_combined(\n",
    "            img_path,\n",
    "            resnet18_by_id[image_id],\n",
    "            target,\n",
    "            model_name=\"ResNet-18\",\n",
    "            class_names=class_names\n",
    "        )\n",
    "        plt.show()\n",
    "        \n",
    "        # Visualize ResNet-50 results\n",
    "        print(\"ResNet-50 model predictions with ground truth:\")\n",
    "        fig2 = visualize_combined(\n",
    "            img_path,\n",
    "            resnet50_by_id[image_id],\n",
    "            target,\n",
    "            model_name=\"ResNet-50\",\n",
    "            class_names=class_names\n",
    "        )\n",
    "        plt.show()\n",
    "        \n",
    "        # Optional: Add a side-by-side comparison of the models for this image\n",
    "        print(f\"Detection counts - ResNet-18: {len(resnet18_by_id[image_id])}, ResNet-50: {len(resnet50_by_id[image_id])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951a04bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nVisualizing results with combined ground truth and predictions...\")\n",
    "create_combined_visualizations(test_dataset, resnet18_results, resnet50_results, num_samples=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b82f35",
   "metadata": {},
   "source": [
    "Side by Side results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4abeca36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_side_by_side(image_path, resnet18_results, resnet50_results, ground_truth, class_names=None):\n",
    "    \"\"\"\n",
    "    Visualize both ResNet-18 and ResNet-50 predictions side by side with ground truth\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the image\n",
    "        resnet18_results: Detection results from ResNet-18 model\n",
    "        resnet50_results: Detection results from ResNet-50 model\n",
    "        ground_truth: Ground truth annotation dictionary\n",
    "        class_names: List of class names for label display\n",
    "    \"\"\"\n",
    "    if class_names is None:\n",
    "        class_names = ['background', 'bird', 'drone']\n",
    "    \n",
    "    # Load image\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Error: Could not load image at {image_path}\")\n",
    "        return None\n",
    "        \n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Create figure with two subplots side by side\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
    "    fig.suptitle(f\"Model Comparison - {os.path.basename(image_path)}\", fontsize=16)\n",
    "    \n",
    "    # Display image in both subplots\n",
    "    ax1.imshow(img)\n",
    "    ax2.imshow(img)\n",
    "    \n",
    "    ax1.set_title(\"ResNet-18 Predictions\")\n",
    "    ax2.set_title(\"ResNet-50 Predictions\")\n",
    "    \n",
    "    # Plot ground truth boxes in green on both plots\n",
    "    gt_boxes = ground_truth[\"boxes\"]\n",
    "    gt_labels = ground_truth[\"labels\"]\n",
    "    \n",
    "    for box, label_idx in zip(gt_boxes, gt_labels):\n",
    "        x1, y1, x2, y2 = box.tolist()\n",
    "        width = x2 - x1\n",
    "        height = y2 - y1\n",
    "        \n",
    "        class_idx = label_idx.item()\n",
    "        class_name = class_names[class_idx] if class_idx < len(class_names) else f\"Class {class_idx}\"\n",
    "        \n",
    "        # Create rectangle patches for ground truth (green)\n",
    "        rect1 = patches.Rectangle((x1, y1), width, height, \n",
    "                               linewidth=2, edgecolor='green', facecolor='none')\n",
    "        rect2 = patches.Rectangle((x1, y1), width, height, \n",
    "                               linewidth=2, edgecolor='green', facecolor='none')\n",
    "        \n",
    "        # Add rectangles to both axes\n",
    "        ax1.add_patch(rect1)\n",
    "        ax2.add_patch(rect2)\n",
    "        \n",
    "        # Add labels to both axes\n",
    "        ax1.text(x1, y1-5, f\"GT: {class_name}\", color='white', \n",
    "                fontsize=9, bbox=dict(facecolor='green', alpha=0.7))\n",
    "        ax2.text(x1, y1-5, f\"GT: {class_name}\", color='white', \n",
    "                fontsize=9, bbox=dict(facecolor='green', alpha=0.7))\n",
    "    \n",
    "    # Plot ResNet-18 predictions in blue on the left plot\n",
    "    for detection in resnet18_results:\n",
    "        x1, y1, width, height = detection['bbox']\n",
    "        class_idx = detection['category_id']\n",
    "        score = detection['score']\n",
    "        \n",
    "        class_name = class_names[class_idx] if class_idx < len(class_names) else f\"Class {class_idx}\"\n",
    "        \n",
    "        # Create rectangle for ResNet-18 prediction\n",
    "        rect = patches.Rectangle((x1, y1), width, height, \n",
    "                               linewidth=2, edgecolor='blue', facecolor='none')\n",
    "        ax1.add_patch(rect)\n",
    "        \n",
    "        # Add label\n",
    "        ax1.text(x1, y1+height+15, f\"{class_name}: {score:.2f}\", color='white', \n",
    "                fontsize=9, bbox=dict(facecolor='blue', alpha=0.7))\n",
    "    \n",
    "    # Plot ResNet-50 predictions in red on the right plot\n",
    "    for detection in resnet50_results:\n",
    "        x1, y1, width, height = detection['bbox']\n",
    "        class_idx = detection['category_id']\n",
    "        score = detection['score']\n",
    "        \n",
    "        class_name = class_names[class_idx] if class_idx < len(class_names) else f\"Class {class_idx}\"\n",
    "        \n",
    "        # Create rectangle for ResNet-50 prediction\n",
    "        rect = patches.Rectangle((x1, y1), width, height, \n",
    "                               linewidth=2, edgecolor='red', facecolor='none')\n",
    "        ax2.add_patch(rect)\n",
    "        \n",
    "        # Add label\n",
    "        ax2.text(x1, y1+height+15, f\"{class_name}: {score:.2f}\", color='white', \n",
    "                fontsize=9, bbox=dict(facecolor='red', alpha=0.7))\n",
    "    \n",
    "    # Add legends\n",
    "    legend_elements1 = [\n",
    "        patches.Patch(edgecolor='green', facecolor='none', linewidth=2, label='Ground Truth'),\n",
    "        patches.Patch(edgecolor='blue', facecolor='none', linewidth=2, label='ResNet-18 Prediction')\n",
    "    ]\n",
    "    legend_elements2 = [\n",
    "        patches.Patch(edgecolor='green', facecolor='none', linewidth=2, label='Ground Truth'),\n",
    "        patches.Patch(edgecolor='red', facecolor='none', linewidth=2, label='ResNet-50 Prediction')\n",
    "    ]\n",
    "    \n",
    "    ax1.legend(handles=legend_elements1, loc='upper right')\n",
    "    ax2.legend(handles=legend_elements2, loc='upper right')\n",
    "    \n",
    "    # Remove axis ticks\n",
    "    ax1.set_xticks([])\n",
    "    ax1.set_yticks([])\n",
    "    ax2.set_xticks([])\n",
    "    ax2.set_yticks([])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)  # Adjust for the suptitle\n",
    "    return fig\n",
    "\n",
    "def create_side_by_side_visualizations(dataset, resnet18_results, resnet50_results, num_samples=5):\n",
    "    \"\"\"\n",
    "    Create side-by-side visualizations comparing ResNet-18 and ResNet-50 predictions\n",
    "    \"\"\"\n",
    "    # Get image paths for easier lookup\n",
    "    img_paths = {}\n",
    "    for img_name in dataset.img_files:\n",
    "        img_path = os.path.join(dataset.img_dir, img_name)\n",
    "        img_paths[img_name] = img_path\n",
    "    \n",
    "    # Group results by image_id\n",
    "    resnet18_by_id = {}\n",
    "    for result in resnet18_results:\n",
    "        image_id = result['image_id']\n",
    "        if image_id not in resnet18_by_id:\n",
    "            resnet18_by_id[image_id] = []\n",
    "        resnet18_by_id[image_id].append(result)\n",
    "    \n",
    "    resnet50_by_id = {}\n",
    "    for result in resnet50_results:\n",
    "        image_id = result['image_id']\n",
    "        if image_id not in resnet50_by_id:\n",
    "            resnet50_by_id[image_id] = []\n",
    "        resnet50_by_id[image_id].append(result)\n",
    "    \n",
    "    # Find image IDs where both models made predictions\n",
    "    common_ids = list(set(resnet18_by_id.keys()) & set(resnet50_by_id.keys()))\n",
    "    \n",
    "    if len(common_ids) == 0:\n",
    "        print(\"No common detections found between models!\")\n",
    "        # Try to find any images with at least one model's predictions\n",
    "        all_ids = list(set(resnet18_by_id.keys()) | set(resnet50_by_id.keys()))\n",
    "        if len(all_ids) > 0:\n",
    "            print(f\"Showing {min(num_samples, len(all_ids))} images with at least one model's predictions\")\n",
    "            common_ids = all_ids\n",
    "        else:\n",
    "            return\n",
    "    \n",
    "    # Select random samples to visualize\n",
    "    if len(common_ids) > num_samples:\n",
    "        sample_ids = np.random.choice(common_ids, size=num_samples, replace=False)\n",
    "    else:\n",
    "        sample_ids = common_ids[:num_samples]\n",
    "    \n",
    "    # Create visualizations for each sample\n",
    "    for idx, image_id in enumerate(sample_ids):\n",
    "        # Get ground truth for this image\n",
    "        image, target = dataset[image_id]\n",
    "        filename = target['orig_filename']\n",
    "        img_path = img_paths[filename]\n",
    "        \n",
    "        # Get results for this image (or empty list if none)\n",
    "        r18_results = resnet18_by_id.get(image_id, [])\n",
    "        r50_results = resnet50_by_id.get(image_id, [])\n",
    "        \n",
    "        print(f\"\\nVisualization {idx+1}/{len(sample_ids)}: Image ID {image_id} ({filename})\")\n",
    "        print(f\"  ResNet-18: {len(r18_results)} detections, ResNet-50: {len(r50_results)} detections\")\n",
    "        \n",
    "        # Create side-by-side visualization\n",
    "        fig = visualize_side_by_side(\n",
    "            img_path,\n",
    "            r18_results,\n",
    "            r50_results,\n",
    "            target,\n",
    "            class_names=class_names\n",
    "        )\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ad7416",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCreating side-by-side visualizations of ResNet-18 and ResNet-50 predictions...\")\n",
    "create_side_by_side_visualizations(test_dataset, resnet18_results, resnet50_results, num_samples=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0550866d",
   "metadata": {},
   "source": [
    "## Summary and Conclusion\n",
    "\n",
    "Summarize the evaluation results and compare the performance of the ResNet-18 and ResNet-50 Faster R-CNN models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60c0bc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to e:/ML/VIP_Cup/results/resnet18_results.json\n",
      "Results saved to e:/ML/VIP_Cup/results/resnet50_results.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def save_results_to_json(results, filename):\n",
    "    \"\"\"Save detection results to a JSON file\"\"\"\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "    print(f\"Results saved to {filename}\")\n",
    "\n",
    "# Save individual model results\n",
    "save_results_to_json(resnet18_results, 'e:/ML/VIP_Cup/results/resnet18_results.json')\n",
    "save_results_to_json(resnet50_results, 'e:/ML/VIP_Cup/results/resnet50_results.json')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
